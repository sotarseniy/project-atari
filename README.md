# **Описание среды**
- В LostLuggage агент должен перемещаться в пространстве и находить потерянные багажи, выпавшие с самолёта.
- За каждый потерянный багаж агент теряет одну жизнь, всего жизней три.
- Цель агента - найти как можно больше багажей, избегая столкновений.

## **Действия**
- 0: NOOP (Ничего не делать)
- 1: UP (Вверх)
- 2: RIGHT (Вправо)
- 3: LEFT (Влево)
- 4: DOWN (Вниз)
- 5: UPRIGHT (Вверх-вправо)
- 6: UPLEFT (Вверх-влево)
- 7: DOWNRIGHT (Вниз-вправо)
- 8: DOWNLEFT (Вниз-влево)

## **Награды**

Один багаж - 3.

![lost-luggage](https://github.com/sotarseniy/project-atari-lost-luggage/blob/main/assets/lost_luggage.gif)

# **Алгоритмы**

## **Q-Learning алгоритм**

**Q-обучение** — один из методов машинного обучения с подкреплением, в котором осуществляется обучение агента на основе значений функции Q, оценивающей полезность различных действий в различных состояниях.
Политика определяется softmax-стратегией. Реализовано с использованием дисконтирования.
На каждом шаге агент обновляет Q-таблицу, используя уравнение Беллмана, учитывая полученные награды.

$Q(s, a) = r + \gamma max(Q(s', a'))$, где 

- $Q(s, a)$ - ожидаемое Q-значение состояния действия;
- $r$ - награда, полученная после выполнения действия $a$ из состояния $s$;
- $\gamma$ - коэффициент дисконтирования, представляющий важность будущих наград;
- $max(Q(s', a'))$ - максимальное Q-значение.

Q-Learning алгоритм может обучаться в неизвестной среде без предварительного знания о ней, что делает его универсальным алгоритмом.

### **Точечные оценки J**

- Математическое ожидание: $34.69$
- Стандартное отклонение: $16.64$

### **Интервальные оценки J**

- С вероятностью $0.9$ математическое ожидание лежит в интервале $[33.21, 36.17]$
- С вероятностью $0.95$ математическое ожидание лежит в интервале $[32.81, 36.57]$