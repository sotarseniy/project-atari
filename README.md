# **Описание среды**
- В LostLuggage агент должен перемещаться в пространстве и находить потерянные багажи, выпавшие с самолёта.
- За каждый потерянный багаж агент теряет одну жизнь, всего жизней три.
- Цель агента - найти как можно больше багажей, избегая столкновений багажа с препятствиями.

## **Действия**
- 0: NOOP (Ничего не делать)
- 1: UP (Вверх)
- 2: RIGHT (Вправо)
- 3: LEFT (Влево)
- 4: DOWN (Вниз)
- 5: UPRIGHT (Вверх-вправо)
- 6: UPLEFT (Вверх-влево)
- 7: DOWNRIGHT (Вниз-вправо)
- 8: DOWNLEFT (Вниз-влево)

## **Награды**

Один багаж - 3 очка.

![lost-luggage](https://github.com/sotarseniy/project-atari-lost-luggage/blob/main/assets/lost_luggage.gif)

# **Вычисление оценки**

Для расчета интервальной оценки для J необходимо сначала вычислить среднее значение J и стандартное отклонение. Затем можно воспользоваться формулой для расчета доверительного интервала: 

$J - \Delta = \overline{X} - T * (S / \sqrt{N})$

$J + \Delta = \overline{X} + T * (S / \sqrt{N})$

где
- J - значение J
- $\Delta$ - точность оценки
- $\overline{X}$ - выборочное срденее
- S - стандартное отклонение
- N - количество эпизодов
- T - критическое значение для выбранного уровня значимости

Для уровня значимости 0.1 критическое значение T = 1.645, а для уровня значимости 0.05 критическое значение T = 1.96.

Для точности $\Delta$ нужно рассчитать количество экспериментов, которые нужно провести, чтобы достичь данной точности, для этого воспользуемся формулой: $N = (T * S / \Delta) ^ 2$

# **Политики**

## **Epsilon Greedy Policy**

Алгоритм, выбирающий наилучшее в данный момент действие, но также добавляет элемент случайности - с некоторой вероятностью выбирается случайное действие для дальнейшего хода.

## **Softmax Policy**

Алгоритм, выбирающий случайные действия с вероятностями, пропорциональными их текущим значениям. Эти политики называются политиками softmax.

# **Алгоритмы**

## **Q-Learning алгоритм**

**Q-обучение** — один из методов машинного обучения с подкреплением, в котором осуществляется обучение агента на основе значений функции Q, оценивающей полезность различных действий в различных состояниях.
Политика определяется softmax-стратегией. Реализовано с использованием дисконтирования.
На каждом шаге агент обновляет Q-таблицу, используя уравнение Беллмана, учитывая полученные награды.

$Q(s, a) = r + \gamma max(Q(s', a'))$, где 

- $Q(s, a)$ - ожидаемое Q-значение состояния действия;
- $r$ - награда, полученная после выполнения действия $a$ из состояния $s$;
- $\gamma$ - коэффициент дисконтирования, представляющий важность будущих наград;
- $max(Q(s', a'))$ - максимальное Q-значение.

Q-Learning алгоритм может обучаться в неизвестной среде без предварительного знания о ней, что делает его универсальным алгоритмом.

### **Точечные оценки J**

- Математическое ожидание: $34.69$
- Стандартное отклонение: $16.64$

### **Интервальные оценки J**

- С вероятностью $0.9$ математическое ожидание лежит в интервале $[33.21, 36.17]$
- С вероятностью $0.95$ математическое ожидание лежит в интервале $[32.81, 36.57]$

Для достижения точности 0.5 нужно провести примерно 4196 экспериметнов.

## **REINFORCE**

Идея алгоритма заключается в том, чтобы обновлять параметры стратегии в направлении градиента логарифма вероятности действия, умноженного на дисконтированное вознаграждение.

Веса нейронной сети обновляются на основе полученного вознаграждения, для этого используется градиентный спуск.

Одним из главных преимуществ алгоритма REINFORCE является его способность работать с дискретными и непрерывными пространствами действий и состояний.

### **Точечные оценки J**

- Математическое ожидание: $25.063$
- Стандартное отклонение: $14.535$
  
### **Интервальные оценки J**

- С вероятностью $0.9$ математическое ожидание лежит в интервале $[24.31, 25.82]$
- С вероятностью $0.95$ математическое ожидание лежит в интервале $[24.16, 25.96]$

Для достижения точности 0.5 нужно провести примерно 3246 экспериметнов.


# **Возможности улучшения**

Использование более сложных и глубоких моделей обучения с подкреплением, таких как deep Q-networks (DQN) для более эффективного и точного обучения.

Внедрение дополнительных элементов и условий в среду LostLuggage, таких как дополнительные задачи или ограничения, для более глубокого и разнообразного обучения агента.